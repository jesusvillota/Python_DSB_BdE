{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Container with white background -->\n",
    "<div style=\"background-color: white; padding: 20px; border-radius: 10px;\">\n",
    "\n",
    "  <!-- Name in bold and approximate LaTeX font style with the logo blue color -->\n",
    "  <h1 style=\"font-family: 'Times New Roman', Times, serif; font-weight: bold; color: #38549c; text-align: center;\">\n",
    "    Introduction to Python | Session #2\n",
    "  </h1>\n",
    "\n",
    "  <!-- Project name in similar style with the logo blue color in italics -->\n",
    "  <h2 style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-style: italic;\">\n",
    "    Diploma in Banking Supervision\n",
    "  </h2>\n",
    "\n",
    "  <!-- CEMFI logo centered -->\n",
    "  <div style=\"text-align: center; margin-bottom: 40px;\">\n",
    "    <img src=\"https://www.cemfi.es/images/Logo-Azul.png\" alt=\"CEMFI Logo\" style=\"width:200px;\">\n",
    "  </div>\n",
    "\n",
    "  <!-- Catchy message about authorship -->\n",
    "  <p style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1.2em;\">\n",
    "    Jesus Villota Miranda © 2025\n",
    "  </p>\n",
    "\n",
    "  <!-- Contact information with logos -->\n",
    "  <p style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1em;\">\n",
    "    Contact:\n",
    "    <a href=\"mailto:jesus.villota@cemfi.edu.es\" style=\"color: #38549c;\">\n",
    "      <img src=\"https://www.logolynx.com/images/logolynx/64/64319177556c729f1806922bcd3adef5.png\" alt=\"Email Logo\" style=\"width: 20px; vertical-align: middle;\">\n",
    "      jesus.villota@cemfi.edu.es\n",
    "    </a> |\n",
    "    <a href=\"https://www.linkedin.com/in/jesusvillotamiranda/\" target=\"_blank\" style=\"color: #38549c;\">\n",
    "      <img src=\"https://1.bp.blogspot.com/-onvhHUdW1Us/YI52e9j4eKI/AAAAAAAAE4c/6s9wzOpIDYcAo4YmTX1Qg51OlwMFmilFACLcBGAsYHQ/s1600/Logo%2BLinkedin.png\" alt=\"LinkedIn Logo\" style=\"width: 20px; vertical-align: middle;\">\n",
    "      LinkedIn\n",
    "    </a>\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data management with Pandas \n",
    "\n",
    "Pandas is a Python library designed for efficient and practical data analysis tasks (clean, manipulate, analyze). Differently from NumPy, which is designed to work with numerical arrays, Pandas is designed to handle relational or labeled data (i.e., data that has been given a context or meaning through labels). \n",
    "\n",
    "Pandas and NumPy share some similarities as Pandas builds on NumPy. However, rather than arrays, the main object for 2D data management in Pandas are `DataFrames` (equivalent to data.frame in R). `DataFrames` are esentially matrices with labeled columns and rows that can accomodate mixed datatypes and missing values. `DataFrames` are composed by rows and columns. Row labels are known as `indices` (start from 0) and columns labels as `columns names`.\n",
    "\n",
    "Pandas is a very good tool for: \n",
    "- Dealing with missing data\n",
    "- Adding or deleting columns\n",
    "- Align data on labels or not on labels (i.e., merging and joining)\n",
    "- Perform grouped operations on data sets. \n",
    "- Transforming other Python data structures to DataFrames.\n",
    "- Multi-indexing hierarchically. \n",
    "- Load data from multiple file tipes. \n",
    "- Handling time-series datas, as it has specific time-series functionalities.\n",
    "\n",
    "\n",
    "**Data structures:**\n",
    "\n",
    "Pandas builds on two main data structures: `series` and `DataFrames`.  `Series` represent 1D arrays while `DataFrames` are 2D labeled arrays.  The easiest way to think about both structures is to conceptualize `DataFrames` as containers of lower dimension data. That is, `DataFrames` columns are composed of `series`, and each of the elements of a `series` (i.e., the rows of the `DataFrame`) are individual scalar (numbers or strings) values. In plain words, `Series` are columns made of scalar elements and `DataFrames` are collections of `Series` that get an assigned label. The image below represents a `DataFrame`\n",
    "\n",
    "<img src=\"./img1.png\" alt=\"df\" width=\"400\"/>\n",
    "\n",
    "\n",
    "All pandas data structures are value-mutable (i.e., we can change the values of elements and replace `DataFrames`) but some are not always size-mutable. The length of a Series cannot be changed, but, for example, columns can be inserted into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Creating a DataFrame. \n",
    "\n",
    "`DataFrames` and `Series` can be created transforming built-in Python datastructures and also by importing data (i.e., reading `.csv` files).\n",
    "\n",
    "Let's start by importing pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example I will create a `DataFrame` from a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Undergrad University",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Fields",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "PhD Desk",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "c9ad0691-4a0b-4264-82c6-20e42688c62c",
       "rows": [
        [
         "0",
         "Joël Marbet",
         "Univeristy of Bern",
         "Monetary economics",
         "20"
        ],
        [
         "1",
         "Alba Miñano-Mañero",
         "University of València",
         "Urban Economics",
         "10"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Undergrad University</th>\n",
       "      <th>Fields</th>\n",
       "      <th>PhD Desk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joël Marbet</td>\n",
       "      <td>Univeristy of Bern</td>\n",
       "      <td>Monetary economics</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alba Miñano-Mañero</td>\n",
       "      <td>University of València</td>\n",
       "      <td>Urban Economics</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name    Undergrad University              Fields  PhD Desk\n",
       "0         Joël Marbet      Univeristy of Bern  Monetary economics        20\n",
       "1  Alba Miñano-Mañero  University of València     Urban Economics        10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phds = {'Name': ['Joël Marbet', 'Alba Miñano-Mañero'], \n",
    "        'Undergrad University': ['Univeristy of Bern', 'University of València'],\n",
    "        'Fields':['Monetary economics','Urban Economics'],\n",
    "        'PhD Desk': [20,10]}\n",
    "phd_df = pandas.DataFrame(phds)\n",
    "phd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how many rows and columns has a dataframe, we can call the `.shape` attribute.  For instance, the dataframe we just created has 2 rows and 4 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phd_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can work with single columns or subset our dataframe to a set of columns by calling the columns names in square braces `[]`. The returning column will be of the `pandas.Series` type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phd_df['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(phd_df['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: If we call it with double braces (`[[]]`)  we'll get a `pandas.DataFrame` rather than a `pandas.Series`. This is so because the inner pair of brackets is generating a list of columns, and the outer allow to select data from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phd_df[['Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(phd_df[['Name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access all columns names by calling the attribute `.columns` of a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phd_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, we can copy a selection of our DataFrame to a new one.  We use the `.copy()` method to generate a copy of the data and avoid broadcasting any change to the original frame we are subsetting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phd_df2 = phd_df[['Name','Fields']].copy()\n",
    "phd_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also create a dataframe from other Python data-structures. In the example below, we create the same `DataFrame` from a list. If we do not specify the column names, it will just give numeric labels to columns from 0 to n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phds_list = [['Joël Marbet', 'Univeristy of Bern','Monetary economics', 20],\n",
    "          ['Alba Miñano-Mañero', 'University of València', 'Urban Economics',10]]\n",
    "# Create a list of column names\n",
    "column_names_list = ['Name', 'Undergraduate University', 'Fields','PhD desk']\n",
    "\n",
    "# Create a DataFrame from the array, using the list as column names and the tuple as row labels\n",
    "df = pandas.DataFrame(phds_list, columns=column_names_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could also be from a tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phds_tu = (('Joël Marbet', 'Univeristy of Bern','Monetary economics', 20),\n",
    "          ('Alba Miñano-Mañero', 'University of València', 'Urban Economics',10))\n",
    "# Create a list of column names\n",
    "column_names_list = ['Name', 'Undergraduate University', 'Fields','PhD desk']\n",
    "\n",
    "# Create a DataFrame from the array, using the list as column names and the tuple as row labels\n",
    "df = pandas.DataFrame(phds_tu, columns=column_names_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(phds_tu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrames` can also receive as data source NumPy arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "data_arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Create a DataFrame from the NumPy array\n",
    "df_arr = pandas.DataFrame(data_arr, columns=['A', 'B', 'C'])\n",
    "\n",
    "df_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can also transform built-in structures into `pandas.Series` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_series= pandas.Series(['Monday','Tuesday','Wednesday']) # from a list\n",
    "week_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous line would be equivalente to create first a list and pass it as argument to `pandas.Series`. We can also specify the row index, which can be very useful for join and concatenate operations and we can also specify a name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_list = ['Monday','Tuesday','Wednesday']\n",
    "week_l_to_series = pandas.Series(week_list,index=range(3,6),name='Week')\n",
    "week_l_to_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could convert that series to a `pandas.DataFrame()` that would have a column name 'Week' and as row index (3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(week_l_to_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create them from dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_s = {\n",
    "    'Monday': 'Work',\n",
    "    'Tuesday': 'Work',\n",
    "    'Wednesday': 'Work',\n",
    "    'Thursday': 'Work',\n",
    "    'Friday': 'Work',\n",
    "    'Saturday': 'Weekend',\n",
    "    'Sunday': 'Weekend'\n",
    "}\n",
    "dic_to_series = pandas.Series(weekdays_s,name='Day_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_to_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Pandas operations and methods return `DataFrames` or `Series`. For instance, the `describe()` method, which returns basic statistics of numerical data returns either a `DataFrame` or a `Series`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr['A'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_arr['A'].describe() # I use describe on a series,  returns a series\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_arr[['A']].describe()  # I use describe on a dataframe, returns a dataframe\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Importing and exporting data \n",
    "\n",
    "Besides creating our own tabular data, we can also use Pandas to import, read and manipulate tabular data. Let's take a look at how to import different data sets. Reading different file types (excel, parquets, json...) is supported  by tha family of `read_*` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_csv ='./card_transdata.csv'\n",
    "data = pandas.read_csv(file_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the first or last *n* elements of the data is possible using the methods `.head(n)` and `.tail(n)`, for the first and bottom elements respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to get all the particular data types specifications of our columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes # it's an attribute of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or get a quick technical summary of our imported data, we can use the `.info()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary is telling us that: \n",
    "- We have a `pandas.DataFrame`. \n",
    "- It has 1,000,000 rows and their index are from 0 to 999,9999\n",
    "- None of the collumns has missing data\n",
    "- All columns data type is a 64 bit float. \n",
    "- It takes up 61 megabytes of our RAM memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the `read_*` families allow to import different sources of data, the `to_*` allows us to export it. In the below example, I am saving  a csv that contains only the first 10 rows of the credit transaction data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10).to_csv('./subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we export other data to the same file, it will typically overwrite the existing one (unless we are specifying sheets in an Excel file, for instance). Also, this happens without warning, so it is important to be consistent in our naming to avoid unwanted overwrites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5).to_csv('./subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Filtering and subsetting dataframes. \n",
    "\n",
    "It is possible to use an approach similar to the selection of columns to be able to subset our dataframe to given rows. \n",
    "\n",
    "For example, let's extract from the credit transaction data those that happened through online orders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = data[data['online_order']==1]\n",
    "online.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that in our new subset the variable **online_order** is always 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online['online_order'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online['online_order'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, to select rows based on a column condition, we first write the condition within `[]`. This selection bracket will give use a series of boolean (i.e., a mask) with True values for those rows that satify the condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data = data['online_order']==1\n",
    "mask_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mask_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, when we embrace that condition with the outer `[]` what we are doing is subsetting the original dataframe to those rows that are True, that is, that satisfy our condition. \n",
    "\n",
    "If our condition involves two discrete values, we can write both conditions within the inner bracket using the `|` separator for *or* conditions and `&` for *and*. That is, we could write `data[(data['var']==x) | (data['var']==y)]`. This line of code is equivalent to using the `.isin()` conditional function with argument `[x,y]`. \n",
    "\n",
    "Sometimes, it is also useful to get rid of missing values on given columns using the `.notna()` method. \n",
    "\n",
    "If we want to extract to select rows and columns simultaneously, we can use the `.loc[condition,variable to keep]`and `.iloc[condition,variable to keep]` operators.  For instance, if we want to extract the distance from home only for the fraudulent transactions we could do: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['fraud']==1,'distance_from_home'] # first element is the condition, second one is the column we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.loc` is also useful to replace the value of columns where a row satisfy certain condition. For instance, let's create a new variable that takes value 1 only for those transactions happenning more than 5km away from home. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['more_5km'] = 0\n",
    "data.loc[data['distance_from_home'] > 5, 'more_5km'] = 1\n",
    "data[data['more_5km']==1]['distance_from_home'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract using indexing of rows and columns we typically rely in `iloc`instead. For instance, in the snipper of code below, I am extracting the rows from 10 to 20 and the second and third column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[10:21,2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In whatever selection you are doing, keep in mind that the reasoning is always you first generate a **mask** and then you get the rows for which the mask holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. Creating new columns\n",
    "\n",
    "Pandas operates with the traditional mathematical operators in an element-wise fashion, without the need of writing for loops. For instance let's convert the distance from kilometers to miles (i.e., we divide by 1.609)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['distance_from_home_km'] = data['distance_from_home']/1.609\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also operate with more than one columns. For instance, let's sum the indicators for repeat retailer and used chip so that we can get a new column that directly tell us if a row satisfies both conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sum_2'] = data['repeat_retailer'] + data['used_chip']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because *sum_2* is not a very intuitive name, let's take advantage of that to show we can rename columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'sum_2':'both_conditions'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5. Merging tables.\n",
    "\n",
    "Sometimes, we find that in the process of cleaning data, we need to put together different dataframes, either vertically or horizontally. This can be achieved concatenating along indexes or, when the dataframes have a common identifier, using `.merge()`\n",
    "\n",
    "Let's start to show we can concatenate along axis. The default axis is 0, which means that it will concatenate rows (i.e., append dataframes vertically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data.iloc[0:100]\n",
    "data_2 = data.iloc[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_rows = pandas.concat([data_1,data_2], axis = 0)\n",
    "concat_rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we try to concatenate horizontally, it will by default try to concatenate with the index (unless we specify ignore_index=True). In the below example, since the columns are subset in  different set of rows, it just fills with NaN the rows that do not share an index in the other dataframe. We can also use custom columns to align the dataframes specifying the column name in the 'keys' options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c1 =data_1[['distance_from_home','repeat_retailer' ]]\n",
    "data_c2 =data_2[['distance_from_last_transaction','used_chip' ]]\n",
    "data_ch = pandas.concat([data_c1,data_c2],axis = 1) \n",
    "data_ch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using concat for horizontal concatenation can be very useful when we have more than one dataset to concatenate and want to avoid repeated lines using `.merge`, which by default can only concatenate two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c1 =data_1[['distance_from_home','repeat_retailer' ]]\n",
    "data_c2 =data_1[['distance_from_last_transaction','used_chip' ]]\n",
    "data_c3 =data_1[['used_pin_number','online_order']]\n",
    "data_ch = pandas.concat([data_c1,data_c2,data_c3],axis = 1) \n",
    "data_ch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we wanted to use the `.merge()` method, we would have to repeat it twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m1 = data_c1.reset_index().merge(data_c2.reset_index(),on='index',how='inner',validate='1:1')\n",
    "data_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m1 = data_m1.merge(data_c3.reset_index(),on='index',how='inner',validate='1:1')\n",
    "data_m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Summary statistics and groupby operations. \n",
    "\n",
    "Pandas also offers various statistical measures that can be applied to numerical data columns. By default, operations exclude missing data and extend across rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean distance from home:', data['distance_from_home'].mean())\n",
    "print('Minimum distance from home:',data['distance_from_home'].min())\n",
    "print('Median distance from home:', data['distance_from_home'].median())\n",
    "print('Maximum distance from home:', data['distance_from_home'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want more flexibility we can also specify a series of statistics and the columns we want them to be computed on using `.agg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.agg(\n",
    "    {\n",
    "           \"distance_from_home\": [\"min\", \"max\", \"median\", \"skew\"],\n",
    "            \"distance_from_last_transaction\": [ \"max\", \"median\", \"mean\"],  \n",
    "       }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: computing the mean of a dummy variable will tell us the percentage of observations within that category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fraud'].mean()  ## 8% are frauds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides this aggreggating operations, we can also compute them within category or by groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"fraud\")[\"distance_from_home\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the default return is a `pandas.Series`, it is useful to convert it to a `pandas.DataFrame` which can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(data.groupby(\"fraud\")[\"distance_from_home\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the new indexing variable is automatically set to our groupby one. We can undo this change to recover the groupby variable as accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(data.groupby(\"fraud\")[\"distance_from_home\"].mean()).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualization: matplotlib and seaborn. \n",
    "\n",
    "The library that will allow us to do data visualization, whether static, animated or interactive, in Python is Matplotlib. \n",
    "\n",
    "Matplotlib plots our data on Figures, each one containing axes where data points are defined as x-y coordinates. This means that matplotlib will allow us to manipulate the displays by changing the elements of these two classes: \n",
    "\n",
    "**Figure**: \n",
    "\n",
    "The entire figure that contains the axes, which are the actual plots. \n",
    "\n",
    "**Axes**:\n",
    "\n",
    "Contains a region for plotting data, and include the x and y axis (and z if is 3D) where we actually plot the data. \n",
    "\n",
    "<img src=\"https://matplotlib.org/stable/_images/anatomy.png\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following examples, we will use data different from the crad transaction. However, just as in most typical real life situations, we will have to do some pre-processing before we can start plotting our data. In this case, we are going to be using data from the [World Health Organization](https://www.who.int/data/gho/data/indicators/indicator-details/GHO/ambient-and-household-air-pollution-attributable-death-rate-(per-100-000-population-age-standardized)) on mortality induced by pollution and country GDP per capita data from the [World Bank](https://ourworldindata.org/grapher/gdp-per-capita-worldbank?tab=table&time=2019..latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_rate = pandas.read_csv('./data.csv')\n",
    "gdp = pandas.read_csv('./gdp-per-capita-worldbank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_rate[(death_rate['Location']=='Algeria') & (death_rate['Dim1']=='Both sexes') & (death_rate['Dim2']=='Total') & (death_rate['IndicatorCode']=='SDGAIRBOD')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the variables we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_vars = ['Indicator','Location','ParentLocationCode','ParentLocation','SpatialDimValueCode','Location','FactValueNumeric']\n",
    "death_rate = death_rate[(death_rate['Dim1']=='Both sexes') & (death_rate['Dim2']=='Total') & (death_rate['IndicatorCode']=='SDGAIRBOD')][keep_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = gdp[(gdp['Year']==2019) & (gdp['Code'].notna()) & (gdp['Code']!='World')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to merge both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_pollution = gdp.merge(death_rate, left_on='Code',right_on='SpatialDimValueCode',how='inner',validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick visualization, we can use the Pandas `.plot()` method, that will generate a Matplotlib figure object. \n",
    "Our sample data is more complicated because we don't have an X axis directly, so we will have a line graph, where the X axis is the index (i.e., row indicator) and the Y axis is the value we are plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_pollution['FactValueNumeric'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is not very informative. Let's exploit the functionalities of the `.plot()` method to get a better grasp of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_pollution.plot.scatter(x='GDP per capita, PPP (constant 2017 international $)',y='FactValueNumeric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatterplot is starting to say something: it seems there is a negative relationship between GDP per capita and deaths due to pollution. We would have been able to produce the same graph to using Matplotlib functionalities as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gdp_pollution['GDP per capita, PPP (constant 2017 international $)'],gdp_pollution['FactValueNumeric'],'o') #'o' = scatterplot\n",
    "plt.xlabel('GDP Per Capita')  \n",
    "plt.ylabel('Air pollution attributable death rate (per 100 000 population)') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Pandas functionality allows us to do direct visualizations, further customizations will be required most of the time to achieve the desired output. This is when Matplotlib comes in handy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (5,5)) # in inches. We can define a converting factor\n",
    "gdp_pollution.plot(ax=ax, x ='GDP per capita, PPP (constant 2017 international $)', y ='FactValueNumeric', kind='scatter')\n",
    "ax.set_xlabel('GDP Per Capita')\n",
    "ax.set_ylabel('Air pollution attributable death rate (per 100 000 population)')\n",
    "ax.set_title('Title of this subplot')\n",
    "x = gdp_pollution['GDP per capita, PPP (constant 2017 international $)'].values\n",
    "y = gdp_pollution['FactValueNumeric'].values\n",
    "f.suptitle('My first graph')\n",
    "f.savefig('./fig1.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to add the labels that indicate the country code using the annotate method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (5,5)) # in inches. We can define a converting factor\n",
    "gdp_pollution.plot(ax=ax, x ='GDP per capita, PPP (constant 2017 international $)', y ='FactValueNumeric', kind='scatter')\n",
    "ax.set_xlabel('GDP Per Capita')\n",
    "ax.set_ylabel('Air pollution attributable death rate (per 100 000 population)')\n",
    "ax.set_title('Title of this subplot')\n",
    "x = gdp_pollution['GDP per capita, PPP (constant 2017 international $)'].values\n",
    "y = gdp_pollution['FactValueNumeric'].values\n",
    "for i in range(len(gdp_pollution)): \n",
    "    plt.annotate(gdp_pollution['Code'][i], (x[i], y[i] + 0.5), fontsize=7)\n",
    "f.suptitle('My first graph')\n",
    "f.savefig('./fig1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot multiple subplots within the same figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f, axs = plt.subplots(nrows=1, ncols=2, figsize = (12,5)) # Now axs contains two elements\n",
    "gdp_pollution.plot(ax=axs[0], x ='GDP per capita, PPP (constant 2017 international $)', y ='FactValueNumeric', kind='scatter')\n",
    "axs[0].set_xlabel('GDP Per Capita')\n",
    "axs[0].set_ylabel('Air pollution attributable death rate (per 100 000 population)')\n",
    "axs[0].set_title('Title of subplot in axs 0')\n",
    "\n",
    "gdp_pollution.plot(ax=axs[1], x ='GDP per capita, PPP (constant 2017 international $)', y ='FactValueNumeric', kind='scatter')\n",
    "axs[1].set_xlabel('GDP Per Capita')\n",
    "axs[1].set_ylabel('Air pollution attributable death rate (per 100 000 population)')\n",
    "axs[1].set_title('Title of subplot in axs 1')\n",
    "\n",
    "f.suptitle('My first figure with two subplots')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform more advanced statistical graphs, we can rely on Seaborn. Seaborn is a library that facilitates the creation of statistical graphics by leveraging matplotlib and integrating with pandas data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions in `seaborn` are classified as: \n",
    "- Figure-level: internally create their own matplotlib figure. When we call this type of functions, they initialize its own figure, so we cannot draw them into an existing axes. To customize its axes, we need to access the Matplotlib axes that are generated within the figure and then add or modify elements. \n",
    "\n",
    "- Axis-levels:  the return plot is a matplotlib.pyplot.Axes object, which means we can use them within the Matplotlib figure set up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=gdp_pollution, x=\"GDP per capita, PPP (constant 2017 international $)\", y=\"FactValueNumeric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Seaborn we can add an additional dimension in the scatterplot by using different colors for observations in different categories specifying the \"hue\" parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=gdp_pollution, x=\"GDP per capita, PPP (constant 2017 international $)\", y=\"FactValueNumeric\", hue = 'ParentLocation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, it is also straightforward to use different markers for each category specifying it in the \"style\" option.  While here we are using the same variable for the differentiation, it is also possible to specify different variables in hue and style. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=gdp_pollution, x=\"GDP per capita, PPP (constant 2017 international $)\", y=\"FactValueNumeric\", hue = 'ParentLocation', style='ParentLocation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore more options, let's merge our data with population at the country level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pandas.read_csv('./population-unwpp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = population[(population['Year']==2019) & (population['Code'].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_pollution = gdp_pollution. merge(population, how='inner', on='Code', validate='1:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make each point have a different size depending on the population of the country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=gdp_pollution, x=\"GDP per capita, PPP (constant 2017 international $)\", y=\"FactValueNumeric\", hue = 'ParentLocation', \n",
    "            size='Population (historical estimates)', sizes = (15,250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.sort_values('Population (historical estimates)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn also has a functionality that allows to draw scatterplots with regression lines (`regplot`). The function `lmplot` allows also to draw the regression lines conditioning on other variables (i.e., by category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=gdp_pollution, x=\"GDP per capita, PPP (constant 2017 international $)\", y=\"FactValueNumeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=gdp_pollution, x=\"GDP per capita, PPP (constant 2017 international $)\", y=\"FactValueNumeric\", hue=\"ParentLocation\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying \"col\" or \"row\" will draw separate graphs for each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=gdp_pollution, x=\"GDP per capita, PPP (constant 2017 international $)\", y=\"FactValueNumeric\", hue=\"ParentLocation\", col =\"ParentLocation\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lmplot` also performs polynomial regressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=gdp_pollution, x=\"GDP per capita, PPP (constant 2017 international $)\", y=\"FactValueNumeric\", order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a similar syntaxis, we can draw histogram and density plots, which can also be helpful to understand the distribution of continuous variables. \n",
    "\n",
    "For instance, in the code below we are going to draw the histogram for GDP per capita for each parent location separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sns.displot(data = gdp_pollution, x = \"GDP per capita, PPP (constant 2017 international $)\",kind = 'hist',col='ParentLocation')\n",
    "a.set_axis_labels(\"GDP\", \"Count\")\n",
    "a.set_titles(\"{col_name}\")\n",
    "a.savefig('./export.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we could have used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "sns.histplot(data = gdp_pollution, x = \"GDP per capita, PPP (constant 2017 international $)\", color=\"skyblue\", label=\"MaxTemp\", kde=True, ax = ax)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('GDP per capita')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmaps are also built-in within Seaborn and are a very popular too. to display the correlation between the variables of the dataframe. It's like visualizing a correlation matrix with colors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(gdp_pollution[[ \"GDP per capita, PPP (constant 2017 international $)\", \"FactValueNumeric\", 'Population (historical estimates)']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice:\n",
    "1. Write a loop that describes all columns of the credit transaction data. \n",
    "2. Write a loop that plots in a different subplot the scatterplot of GDP vs mortality for each Parent Location. It should have 3 columns and 2 rows. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
